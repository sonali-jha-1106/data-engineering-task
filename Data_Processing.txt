import requests
import pandas as pd
from tenacity import retry, stop_after_attempt, wait_fixed

parquet_url = 'https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page'
local_parquet_path = 'file.parquet'
output_csv_path = 'filtered_2019.csv'
aggregated_csv_path = 'aggregated_2019.csv'

@retry(stop=stop_after_attempt(3), wait=wait_fixed(2))
def download_parquet_file(url, local_path):
 response = requests.get(url, stream=True)
 response.raise_for_status()
 with open(local_path, 'wb') as file:
 for chunk in response.iter_content(chunk_size=8192):
 file.write(chunk)
 print(f'Downloaded {url} to {local_path}')

def process_data(df):
 df = df.dropna()
 df = df[df['trip_distance'] > 0]
 df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])
 df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])
 df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60 # duration in minutes
 df['average_speed'] = df['trip_distance'] / (df['trip_duration'] / 60) # speed in distance units per hour
 return df

def filter_and_save_csv(parquet_path, output_path):
 df = pd.read_parquet(parquet_path)
 df = df[df['year'] == 2019]
 df = process_data(df)
 df.to_csv(output_path, index=False)
 print(f'Saved filtered and processed data to {output_path}')
 return df

def aggregate_and_save_csv(df, output_path):
 df['date'] = df['pickup_datetime'].dt.date
 aggregated_df = df.groupby('date').agg({
 'trip_id': 'count', # Assuming 'trip_id' is a unique identifier for each trip
 'fare_amount': 'mean'
 }).rename(columns={'trip_id': 'total_trips', 'fare_amount': 'average_fare'})
 aggregated_df.to_csv(output_path, index=True)
 print(f'Saved aggregated data to {output_path}')

try:
 download_parquet_file(parquet_url, local_parquet_path)
 filtered_df = filter_and_save_csv(local_parquet_path, output_csv_path)
 aggregate_and_save_csv(filtered_df, aggregated_csv_path)
except Exception as e:
 print(f'Failed to process the Parquet file: {e}')
